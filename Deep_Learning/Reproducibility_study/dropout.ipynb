{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR2FKOkfR27I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import decimal\n",
        "import os\n",
        "import csv\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RH830SXTFqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def findValueN(prob):\n",
        "    # pn = 256 for 2 first hidden layers\n",
        "    # pn = 512 for last layer\n",
        "        # n values are the number of hidden units at each layer\n",
        "    n1 = 256.0/prob\n",
        "    n2 = 256.0/prob\n",
        "    n3 = 512.0/prob\n",
        "    n = [n1,n2,n3,prob]\n",
        "        # return number o\n",
        "    float(n[3])\n",
        "    return n\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGgjiNOvTF6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the archetecture for constant archetecture\n",
        "def addPValue(prob):\n",
        "        # layers all have 2048 hidden units as described in the research paper\n",
        "    n1 = 2048\n",
        "    n2 = 2048\n",
        "    n3 = 2048\n",
        "    n = [n1,n2,n3,prob]\n",
        "    float(n[3])\n",
        "    return n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASrXpJyLTF94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Runs dropout in an neural network for specific layer values\n",
        "def runDropout(*layer):\n",
        "        # Sequential model and the layers that describe the model\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape, activation=\"relu\"))\n",
        "    # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
        "    model.add(Dense(layer[0], activation=tf.nn.relu))\n",
        "    model.add(Dropout(layer[3]))\n",
        "    model.add(Dense(layer[1], activation=tf.nn.relu))\n",
        "    model.add(Dropout(layer[3]))\n",
        "    model.add(Dense(layer[2], activation=tf.nn.relu))\n",
        "    model.add(Dropout(layer[3]))\n",
        "    model.add(Dense(10, activation=tf.nn.softmax))\n",
        "\n",
        "\n",
        "        # Configure model before training\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "        # Train the model for a fixed number of epochs\n",
        "    history_dropout = model.fit(x=x_train, y=y_train, validation_split=0.1, epochs=10, batch_size=32)\n",
        "\n",
        "        # Training accuracy\n",
        "    accuracy = history_dropout.history['accuracy']\n",
        "    train_err = 100.0 - 100.0*(accuracy[-1])\n",
        "\n",
        "        # Test Accuracy\n",
        "            # Validation accuracy possibly not same as Test accuracy\n",
        "    val_acc = history_dropout.history['val_accuracy']\n",
        "    test_err = 100.0 - 100.0*(val_acc[-1])\n",
        "    # *** Calculate error bars HERE ***\n",
        "    finalError = [test_err, train_err, layer[3]]\n",
        "    return finalError\n",
        "    # END OF RUN DROPOUT #\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuPCnU7PTsWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate all p values in a float range\n",
        "def floatRange(start, stop, step):\n",
        "  while start <= stop:\n",
        "    yield float(start)\n",
        "    start += decimal.Decimal(step)\n",
        "\n",
        "def runForAllP():\n",
        "    # lists to store ploting values locally\n",
        "        # figure a error\n",
        "    a_test_error = []\n",
        "    a_train_error = []\n",
        "        # figure b error\n",
        "    b_test_error = []\n",
        "    b_train_error = []\n",
        "        # returns the list of p values from 0.1-1.0, into list\n",
        "    pValues = list(floatRange(0, 0.9, '0.1'))\n",
        "        # remove 0 from the pValue list\n",
        "    del pValues[0]\n",
        "    pValues.append(0.99)\n",
        "    print(pValues)\n",
        "    # run dropout on all values of p for both figures\n",
        "    for i in pValues:\n",
        "        # check if pValue was already calculated\n",
        "            # if so, import values and don't run\n",
        "            # Currently running without storing/checking for previous runs\n",
        "        varyLayer = findValueN(i)\n",
        "        constLayer = addPValue(i)\n",
        "            # Store error values for figure a\n",
        "        errorFigA = runDropout(*constLayer)\n",
        "        a_test_error.append(errorFigA[0])\n",
        "        a_train_error.append(errorFigA[1])\n",
        "        print(a_train_error)\n",
        "        print(\"\\n\")\n",
        "    for i in pValues:\n",
        "        varyLayer = findValueN(i)\n",
        "        constLayer = addPValue(i)\n",
        "        errorFigB = runDropout(*varyLayer)\n",
        "        # Store error values for figure b\n",
        "        b_test_error.append(errorFigB[0])\n",
        "        b_train_error.append(errorFigB[1])\n",
        "    # Figure 9a\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.ylabel('Classification Error %')\n",
        "    plt.xlabel('Probability of retaining a unit (p)')\n",
        "    plt.yticks(np.arange(0, 4, 0.5))\n",
        "    plt.plot(pValues, a_test_error, label = \"Test Error\")\n",
        "    plt.plot(pValues, a_train_error, label = \"Training Error\")\n",
        "    plt.legend(loc='upper right')\n",
        "    # Figure 9b\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.ylabel('Classification Error %')\n",
        "    plt.xlabel('Probability of retaining a unit (p)')\n",
        "    plt.yticks(np.arange(0, 3.5, 0.5))\n",
        "    plt.plot(pValues, b_test_error, label = \"Test Error\")\n",
        "    plt.plot(pValues, b_train_error, label = \"Training Error\")\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.tight_layout()\n",
        "    # save plotted dropout figure\n",
        "    plt.savefig('dropout-figure.png')\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsQCaH9ZTsZ5",
        "colab_type": "code",
        "outputId": "f843ea4b-9539-4aba-867f-692eb3730d47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Load dataket to appropriate valiables\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Reshaping the array to 4-dims so that it can work with the API\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "input_shape = (28, 28, 1)\n",
        "# Set values to float so that we can get decimal points after division\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "# Normalizing the RGB codes by dividing it to the max RGB value\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2ZwE1PfUPf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY5EaQW2UPwb",
        "colab_type": "code",
        "outputId": "15ba3b6d-e0d3-4a86-8e14-6078ed9b09ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# runs both archetectures for all values of p (from 0.1-1.0, inclusive)\n",
        "# Currently takes too long to run, but I'm confident that the output will come\n",
        "    # close to the results in the figure\n",
        "runForAllP()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]\n",
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 743s 440ms/step - loss: 0.1740 - accuracy: 0.9494 - val_loss: 0.0668 - val_accuracy: 0.9822\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 740s 439ms/step - loss: 0.0617 - accuracy: 0.9831 - val_loss: 0.0650 - val_accuracy: 0.9812\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 740s 438ms/step - loss: 0.0372 - accuracy: 0.9905 - val_loss: 0.0759 - val_accuracy: 0.9813\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 745s 441ms/step - loss: 0.0263 - accuracy: 0.9932 - val_loss: 0.0904 - val_accuracy: 0.9827\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 742s 440ms/step - loss: 0.0217 - accuracy: 0.9948 - val_loss: 0.0784 - val_accuracy: 0.9845\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 745s 441ms/step - loss: 0.0202 - accuracy: 0.9957 - val_loss: 0.1117 - val_accuracy: 0.9835\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 743s 440ms/step - loss: 0.0197 - accuracy: 0.9956 - val_loss: 0.0961 - val_accuracy: 0.9850\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 733s 434ms/step - loss: 0.0146 - accuracy: 0.9972 - val_loss: 0.0913 - val_accuracy: 0.9843\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 738s 437ms/step - loss: 0.0138 - accuracy: 0.9971 - val_loss: 0.1083 - val_accuracy: 0.9862\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 745s 441ms/step - loss: 0.0168 - accuracy: 0.9971 - val_loss: 0.0982 - val_accuracy: 0.9865\n",
            "[0.29259324073791504]\n",
            "\n",
            "\n",
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 753s 446ms/step - loss: 0.1695 - accuracy: 0.9503 - val_loss: 0.0704 - val_accuracy: 0.9802\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 753s 446ms/step - loss: 0.0626 - accuracy: 0.9833 - val_loss: 0.0676 - val_accuracy: 0.9838\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 748s 443ms/step - loss: 0.0399 - accuracy: 0.9892 - val_loss: 0.1191 - val_accuracy: 0.9797\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 751s 445ms/step - loss: 0.0312 - accuracy: 0.9924 - val_loss: 0.0590 - val_accuracy: 0.9863\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 752s 445ms/step - loss: 0.0238 - accuracy: 0.9948 - val_loss: 0.0780 - val_accuracy: 0.9842\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 757s 448ms/step - loss: 0.0212 - accuracy: 0.9952 - val_loss: 0.0837 - val_accuracy: 0.9843\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 752s 445ms/step - loss: 0.0189 - accuracy: 0.9956 - val_loss: 0.1602 - val_accuracy: 0.9798\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 749s 444ms/step - loss: 0.0197 - accuracy: 0.9961 - val_loss: 0.1014 - val_accuracy: 0.9862\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 747s 442ms/step - loss: 0.0179 - accuracy: 0.9966 - val_loss: 0.0955 - val_accuracy: 0.9818\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 761s 451ms/step - loss: 0.0168 - accuracy: 0.9970 - val_loss: 0.0980 - val_accuracy: 0.9857\n",
            "[0.29259324073791504, 0.2962946891784668]\n",
            "\n",
            "\n",
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 747s 442ms/step - loss: 0.1826 - accuracy: 0.9470 - val_loss: 0.0691 - val_accuracy: 0.9803\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 744s 441ms/step - loss: 0.0682 - accuracy: 0.9810 - val_loss: 0.0633 - val_accuracy: 0.9833\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 750s 444ms/step - loss: 0.0442 - accuracy: 0.9882 - val_loss: 0.0603 - val_accuracy: 0.9862\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 752s 445ms/step - loss: 0.0348 - accuracy: 0.9917 - val_loss: 0.0739 - val_accuracy: 0.9848\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 756s 448ms/step - loss: 0.0251 - accuracy: 0.9937 - val_loss: 0.1045 - val_accuracy: 0.9817\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 746s 442ms/step - loss: 0.0261 - accuracy: 0.9941 - val_loss: 0.0936 - val_accuracy: 0.9840\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 746s 442ms/step - loss: 0.0212 - accuracy: 0.9950 - val_loss: 0.1066 - val_accuracy: 0.9845\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 747s 443ms/step - loss: 0.0183 - accuracy: 0.9962 - val_loss: 0.1807 - val_accuracy: 0.9797\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 752s 445ms/step - loss: 0.0208 - accuracy: 0.9958 - val_loss: 0.1132 - val_accuracy: 0.9862\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 750s 444ms/step - loss: 0.0218 - accuracy: 0.9964 - val_loss: 0.1028 - val_accuracy: 0.9833\n",
            "[0.29259324073791504, 0.2962946891784668, 0.3574073314666748]\n",
            "\n",
            "\n",
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 749s 444ms/step - loss: 0.1926 - accuracy: 0.9431 - val_loss: 0.0685 - val_accuracy: 0.9803\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 744s 441ms/step - loss: 0.0794 - accuracy: 0.9787 - val_loss: 0.0617 - val_accuracy: 0.9840\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 739s 438ms/step - loss: 0.0536 - accuracy: 0.9863 - val_loss: 0.0599 - val_accuracy: 0.9848\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 747s 443ms/step - loss: 0.0438 - accuracy: 0.9891 - val_loss: 0.1033 - val_accuracy: 0.9795\n",
            "Epoch 5/10\n",
            " 468/1688 [=======>......................] - ETA: 8:43 - loss: 0.0287 - accuracy: 0.9924Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}